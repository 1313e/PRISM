{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1: Basic usage\n",
    "In this tutorial, the basic functionalities of the *PRISM* package are demonstrated and (somewhat) serve as a minimal example on how to use the package.\n",
    "All functionality discussed here is described in more detail in the [online documentation](https://prism-tool.readthedocs.io/en/latest).\n",
    "To make sure that everything is working properly, let's check first if *PRISM* can be imported correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import prism\n",
    "prism.get_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If executing the cell above results in no exceptions and shows a nice overview of the current configuration and requirements, then we are good to go.\n",
    "It might be preferable to remove all *PRISM* working directories from the current directory as well, as this tutorial assumes they do not exist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & initialization\n",
    "In order to use the *PRISM* pipeline, one requires two things: the `Pipeline` class and a `ModelLink` subclass.\n",
    "The `Pipeline` class is the main user class of the *PRISM* package and provides a user-friendly interface/environment that gives access to all operations within the pipeline.\n",
    "It can be seen as the \"conductor\" of the *PRISM* package, as it governs all other objects and orchestrates their communications and method calls.\n",
    "It is linked to a model by a user-written `ModelLink` subclass object, which allows the `Pipeline` to extract all necessary model information and call the model.\n",
    "For this tutorial, we will use one of *PRISM*'s basic `ModelLink` subclasses (the `GaussianLink` class).\n",
    "See [ModelLink subclasses](./2_modellink_subclasses.ipynb) for more information on the `ModelLink` class and how to make a subclass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let's import the `Pipeline` and `GaussianLink` classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prism import Pipeline\n",
    "from prism.modellink import GaussianLink"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have imported these two classes, we should initialize our `ModelLink` subclass, the `GaussianLink` class in this case.\n",
    "In addition to user-defined arguments, every `ModelLink` subclass takes two optional arguments, *model_parameters* and *model_data*.\n",
    "The use of either one will add the provided parameters/data to the default parameters/data defined in the class.\n",
    "Since the `GaussianLink` class does not have default data defined, it is required to supply it with some data constraints during initialization (using an array, dict or external file)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let's define some data and initialize the `GaussianLink` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = {3: [3.0, 0.1],    # f(3) = 3.0 +- 0.1\n",
    "              5: [5.0, 0.1],    # f(5) = 5.0 +- 0.1\n",
    "              7: [3.0, 0.1]}    # f(7) = 3.0 +- 0.1\n",
    "modellink_obj = GaussianLink(model_data=model_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the representation of the created `GaussianLink` object to see what exactly was used to initialize it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ModelLink object representation:\")\n",
    "print('-'*32)\n",
    "print(repr(modellink_obj))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see that the created `ModelLink` object used the `GaussianLink` class and that it was initialized using the same data values as we provided, as expected.\n",
    "It also shows the default parameter values that were used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Pipeline` class takes a mandatory `ModelLink` object and several optional arguments, which are mostly paths and the type of `Emulator` class that must be used.\n",
    "As we already have our `ModelLink` object and we want to use the standard paths and `Emulator`, we can initialize the `Pipeline`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(modellink_obj)\n",
    "print(\"Pipeline object representation:\")\n",
    "print('-'*31)\n",
    "print(repr(pipe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see that the `Pipeline` class was initialized using our `ModelLink` object.\n",
    "Also, as we had not provided it and none existed yet, *PRISM* has created a working directory to store the emulator in (`./prism_0`).\n",
    "If a working directory had already existed (due to previous runs), *PRISM* would have automatically attempted to load the last one that was created that starts with the given *prefix* (optional argument).\n",
    "If no errors are raised during the initialization of the `Pipeline` class, *PRISM* is ready to start emulating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic user-methods\n",
    "A `Pipeline` object has 6 user-methods:\n",
    "- `analyze()`: Analyzes the current emulator iteration;\n",
    "- `construct()`: Constructs the specified emulator iteration;\n",
    "- `details()`: Prints detailed overview of specified iteration;\n",
    "- `evaluate(sam_set)`: Evaluates *sam_set* in specified iteration;\n",
    "- `project()`: Creates projections for specified iteration;\n",
    "- `run()`: Runs a full cycle of the pipeline for specified iteration (can also be done using `pipe()`).\n",
    "\n",
    "All user-methods besides `evaluate()` solely take optional input arguments.\n",
    "In case an optional argument is not provided, the `Pipeline` assumes the most logical value.\n",
    "The most important argument, the emulator iteration (*emul_i*), is assumed to be the last (`analyze()`, `evaluate()`, `project()`), next (`construct()`, `run()`) or current (`details()`) iteration.\n",
    "'last' here refers to the latest iteration that has been fully constructed, while 'current' refers to the latest iteration (regardless of status).\n",
    "Note that `analyze()` does not take *emul_i* as a valid input.\n",
    "\n",
    "Below, we discuss the basic functionalities of these 6 user-methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### construct() & details()\n",
    "The most important user-method in the `Pipeline` is the `construct()`-method.\n",
    "The `construct()`-method allows for the specified emulator iteration to be constructed, or the next iteration if we do not specify it.\n",
    "When constructing an emulator iteration, the `Pipeline` class will call the model wrapped in the provided `ModelLink` subclass (which is a Gaussian model in this case) with a pre-determined number of evaluations.\n",
    "This number is either the initial number (if first iteration) or the number of plausible samples in the previous iteration (if not first iteration).\n",
    "As we currently have not constructed anything yet and did not modify any of the default values, the initial number should be the default number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.n_sam_init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After evaluating this number of samples in the model, the `Pipeline` will instruct the `Emulator` (which was automatically initialized by the `Pipeline` class) to construct the first iteration of the emulator.\n",
    "This involves many different calculations and operations, but a summary would be that it determines all active parameters; obtains the best-fitting polynomial terms; calculates the Gaussian variances and marks the current iteration as completed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know what this method does, let's construct the next iteration of the emulator, which should be the first one (skip this step if one has already done this before):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.construct()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If no emulator existed yet, then *PRISM* will have constructed the first iteration of the emulator.\n",
    "We can check that this is the case by attempting to construct the first iteration again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.construct(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we asked for the first iteration to be constructed, but as it is already finished, *PRISM* will simply state this and immediately return."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the construction is completed, the `Pipeline` automatically calls the `details()`-method to provide an overview of the current status of the iteration.\n",
    "This is done after all user-methods besides `evaluate()` and after initializing the `Pipeline` object (if a working directory already existed when it was initialized above, then we would see it as well).\n",
    "\n",
    "This overview provides us with many details about the specified iteration, including what emulator is currently loaded; its construction, analysis and projection statuses; and the parameter space.\n",
    "If the specified iteration has not finished constructing yet (because it was interrupted for example), the overview instead lists what components are still missing before construction completion.\n",
    "Finishing an interrupted construction process is done in the exact same way as constructing an entirely new iteration; by calling the `construct()`-method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### analyze()\n",
    "By default, the emulator iteration is analyzed immediately after it has been constructed, which we could have disabled by passing *analyze=False* to `construct()` above.\n",
    "Before we explain what analyzing does, let's make sure that the last iteration is analyzed by (re)analyzing it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.analyze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When calling `analyze()`, the `Pipeline` will generate a large Latin-Hypercube design of proposed samples and evaluate them in the emulator at the last iteration.\n",
    "Using the implausibility parameters that have been provided to the `Pipeline`, it will determine which samples are considered \"plausible\" and should be used for constructing the next emulator iteration.\n",
    "If the number of plausible samples could be problematic, the `Pipeline` will either block the construction of the next iteration (if it is definitely too low) or raise a warning about it (if it is potentially too low), with the latter probably being shown for our case.\n",
    "An emulator iteration must have been analyzed first before the next iteration can be constructed.\n",
    "If we were to attempt to construct the next iteration without analyzing the last first, `Pipeline` will simply call `analyze()` first before construction.\n",
    "\n",
    "Analyzing an emulator iteration also has a different effect: it sets the implausibility parameters for that iteration, which consist of the implausibility cut-offs and wildcards.\n",
    "The implausibility cut-offs determine what the maximum implausibility values are that an evaluated sample is allowed to have, to be considered 'plausible'.\n",
    "For example, if the implausibility cut-offs are $[4.0, 3.5, 3.2]$ and a sample evaluated in the emulator has implausibility values $[3.8, 3.1, 3.7]$, then this sample would be marked as 'implausible', as the second-highest implausibility value is higher than the second-highest implausibility cut-off ($3.7 > 3.5$).\n",
    "However, if one would have added an implausibility wildcard to the analysis, which ignores the highest implausibility value, then this sample would be marked as 'plausible' (as $3.7 \\leq 4.0$ and $3.1 \\leq 3.5$).\n",
    "\n",
    "Before the analysis, the implausibility parameters can be changed freely (which affects the `evaluate()` and `project()`-methods).\n",
    "After the analysis however, they can only be changed by reanalyzing the iteration itself with different parameters.\n",
    "This is to make sure that the used parameters throughout an iteration are consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check what the current implausibility parameters are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Implausibility cut-offs: %s\" % (pipe.impl_cut[1]))\n",
    "print(\"# of cut-off wildcards: %i\" % (pipe.cut_idx[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's change the implausibility parameters to ``[4, 3.5, 3.2]`` with a single wildcard, by providing this information to the *impl_cut* argument of `analyze()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.analyze(impl_cut=[0, 4, 3.5, 3.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check if the implausibility parameters have been updated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Implausibility cut-offs: %s\" % (pipe.impl_cut[1]))\n",
    "print(\"# of cut-off wildcards: %i\" % (pipe.cut_idx[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will have permanently changed the implausibility parameters for the first emulator iteration, and will automatically be used by the other methods that require them.\n",
    "If this iteration had not been analyzed yet, we could have set the implausibility parameters the same way or by using (which will raise an error now):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.impl_cut=[0, 4, 3.5, 3.2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate(sam_set)\n",
    "Although the emulator is mostly evaluated internally, we can also evaluate specified sample sets in the emulator using the `evaluate()`-method.\n",
    "Requested sample sets can be provided in either an array format or a dict.\n",
    "For example, let's evaluate a sample with 1s for all parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.evaluate([1, 1, 1])                    # Using an array_like\n",
    "print('\\n')\n",
    "pipe.evaluate({'A1': 1, 'B1': 1, 'C1': 1})  # Using a dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that the answers of both lines are the same.\n",
    "When using an array as the input, the parameter values are assumed to be sorted alphabetically on parameter name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we provide a 2D sample set to `evaluate()`, it will no longer print the result, but instead return it to us in a dict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pipe.evaluate([[1, 1, 1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allows us to potentially pipe the results into a different processing pipeline if we wish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### project()\n",
    "In order to see how the emulator is doing, we need a method that allows us to visualize its performance.\n",
    "This is done by creating so-called *projection figures* of all active parameters in the model.\n",
    "Each projection figure consists out of two subplots: the minimum implausibility and the line-of-sight depth.\n",
    "These two subplots show us where the best and the most plausible samples can be found in parameter space, respectively.\n",
    "As projection figures have many different properties and options, please see the [online documentation](https://prism-tool.readthedocs.io/en/latest/user/using_prism.html#projections) for a detailed description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, *PRISM* will create projection figures at decent resolutions, but it can easily take a few minutes per figure to make them, as many calculations are required.\n",
    "So, before creating the projection figures, we will set the resolution a bit lower (from 25x25x250 to 15x15x75):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.proj_res = 15\n",
    "pipe.proj_depth = 75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, *PRISM* will create 2D and 3D projection figures of all parameter combinations in the model for the last iteration, which can be made using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.project()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure that *PRISM* can run on most systems, it never shows the plots it creates.\n",
    "Instead, we can find the created plots in the corresponding working directory (which is given in the `details()` overview given above).\n",
    "\n",
    "The `project()`-method takes many different keyword arguments, whose descriptions and effects can be found in the online documentation mentioned earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run()\n",
    "The last user-method in the `Pipeline` class, is the `run()`-method.\n",
    "This method is basically nothing more than an accessibility method that allows us to run a full cycle of the `Pipeline`.\n",
    "It can also be accessed by calling the `Pipeline` object directly (e.g., `pipe()` instead of `pipe.run()`).\n",
    "Running a full cycle involves constructing an emulator iteration, analyzing it and making all projection figures for it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let's say that we want to run a full cycle of the `Pipeline` for the second emulator iteration.\n",
    "Then, instead of calling all methods separately like before, we can simply do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.run(2) # Or pipe(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executing this is equivalent to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.construct(2)\n",
    "pipe.project(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the way the `construct()`-method is programmed, if the emulator iteration is already fully constructed, it will only call the `analyze()`-method if it has not been analyzed before and *analyze=True* is given.\n",
    "This is probably more noticeable when using `run()` than when using `construct()`.\n",
    "The reason for this is to make sure that using `run()` (or `construct()`) in an external pipeline cannot reanalyze an emulator iteration without being explicitly told to do so."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
